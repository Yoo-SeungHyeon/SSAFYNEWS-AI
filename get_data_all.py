#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SSAFY News PostgreSQL ë°ì´í„° CSV ë‚´ë³´ë‚´ê¸° í”„ë¡œê·¸ë¨
ë°ëª¨ ë°ì´í„° ìƒì„±ìš©

ì‚¬ìš©ë²•:
python get_data_all.py
"""

import psycopg2
import pandas as pd
import numpy as np
import os
import json
from datetime import datetime
import sys

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
DB_CONFIG = {
    'host': 'localhost',
    'port': '5433',
    'database': 'news',
    'user': 'ssafynews',
    'password': 'ssafynews13'
}

# CSV ì¶œë ¥ ë””ë ‰í† ë¦¬
OUTPUT_DIR = 'exported_data'

def create_output_directory():
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {OUTPUT_DIR}")

def connect_to_db():
    """PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        print("âœ… PostgreSQL ì—°ê²° ì„±ê³µ")
        return conn
    except psycopg2.Error as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

def export_news_articles(conn):
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
    print("\nğŸ“° ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° ë‚´ë³´ë‚´ëŠ” ì¤‘...")
    
    query = """
    SELECT 
        news_id,
        title,
        author,
        link,
        summary,
        updated,
        full_text,
        category,
        keywords,
        CASE 
            WHEN embedding IS NOT NULL THEN 'TRUE'
            ELSE 'FALSE'
        END as has_embedding
    FROM news_api_newsarticle
    ORDER BY updated DESC;
    """
    
    try:
        df = pd.read_sql_query(query, conn)
        
        # ë‚ ì§œ í¬ë§·íŒ…
        df['updated'] = pd.to_datetime(df['updated']).dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # í‚¤ì›Œë“œ ì •ë¦¬
        df['keywords'] = df['keywords'].fillna('')
        df['keywords'] = df['keywords'].str.replace('{', '').str.replace('}', '').str.replace('"', '')
        
        # CSV ì €ì¥
        filename = f"{OUTPUT_DIR}/news_articles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ë‰´ìŠ¤ ê¸°ì‚¬ {len(df)}ê°œ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename}")
        return df
        
    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        return None

def export_user_data(conn):
    """ì‚¬ìš©ì ë°ì´í„° ë‚´ë³´ë‚´ê¸° (ê°œì¸ì •ë³´ ì œì™¸)"""
    print("\nğŸ‘¤ ì‚¬ìš©ì ë°ì´í„° ë‚´ë³´ë‚´ëŠ” ì¤‘...")
    
    query = """
    SELECT 
        id as user_id,
        username,
        email,
        is_active,
        date_joined,
        last_login
    FROM accounts_user
    ORDER BY date_joined DESC;
    """
    
    try:
        df = pd.read_sql_query(query, conn)
        
        # ì´ë©”ì¼ ë§ˆìŠ¤í‚¹ (ê°œì¸ì •ë³´ ë³´í˜¸)
        df['email_masked'] = df['email'].apply(lambda x: x[:3] + '*' * (len(x.split('@')[0]) - 3) + '@' + x.split('@')[1] if '@' in str(x) else str(x))
        df = df.drop(['email'], axis=1)
        
        # ë‚ ì§œ í¬ë§·íŒ…
        df['date_joined'] = pd.to_datetime(df['date_joined']).dt.strftime('%Y-%m-%d %H:%M:%S')
        df['last_login'] = pd.to_datetime(df['last_login']).dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # CSV ì €ì¥
        filename = f"{OUTPUT_DIR}/users_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ì‚¬ìš©ì {len(df)}ëª… ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename}")
        return df
        
    except Exception as e:
        print(f"âŒ ì‚¬ìš©ì ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        return None

def export_likes_data(conn):
    """ì¢‹ì•„ìš” ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
    print("\nâ¤ï¸ ì¢‹ì•„ìš” ë°ì´í„° ë‚´ë³´ë‚´ëŠ” ì¤‘...")
    
    query = """
    SELECT 
        l.id as like_id,
        l.user_id,
        l.news_id,
        l.created_at,
        n.title as news_title,
        n.category,
        u.username
    FROM news_api_like l
    JOIN news_api_newsarticle n ON l.news_id = n.news_id
    JOIN accounts_user u ON l.user_id = u.id
    ORDER BY l.created_at DESC;
    """
    
    try:
        df = pd.read_sql_query(query, conn)
        
        # ë‚ ì§œ í¬ë§·íŒ…
        df['created_at'] = pd.to_datetime(df['created_at']).dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # CSV ì €ì¥
        filename = f"{OUTPUT_DIR}/likes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ì¢‹ì•„ìš” {len(df)}ê°œ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename}")
        return df
        
    except Exception as e:
        print(f"âŒ ì¢‹ì•„ìš” ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        return None

def export_views_data(conn):
    """ì¡°íšŒ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
    print("\nğŸ‘ï¸ ì¡°íšŒ ë°ì´í„° ë‚´ë³´ë‚´ëŠ” ì¤‘...")
    
    query = """
    SELECT 
        v.id as view_id,
        v.user_id,
        v.news_id,
        v.viewed_at,
        n.title as news_title,
        n.category,
        u.username
    FROM news_api_view v
    JOIN news_api_newsarticle n ON v.news_id = n.news_id
    JOIN accounts_user u ON v.user_id = u.id
    ORDER BY v.viewed_at DESC;
    """
    
    try:
        df = pd.read_sql_query(query, conn)
        
        # ë‚ ì§œ í¬ë§·íŒ…
        df['viewed_at'] = pd.to_datetime(df['viewed_at']).dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # CSV ì €ì¥
        filename = f"{OUTPUT_DIR}/views_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ì¡°íšŒ {len(df)}ê°œ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename}")
        return df
        
    except Exception as e:
        print(f"âŒ ì¡°íšŒ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        return None

def export_comments_data(conn):
    """ëŒ“ê¸€ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
    print("\nğŸ’¬ ëŒ“ê¸€ ë°ì´í„° ë‚´ë³´ë‚´ëŠ” ì¤‘...")
    
    query = """
    SELECT 
        c.id as comment_id,
        c.user_id,
        c.news_id,
        c.content,
        c.created_at,
        n.title as news_title,
        n.category,
        u.username
    FROM news_api_comment c
    JOIN news_api_newsarticle n ON c.news_id = n.news_id
    JOIN accounts_user u ON c.user_id = u.id
    ORDER BY c.created_at DESC;
    """
    
    try:
        df = pd.read_sql_query(query, conn)
        
        # ë‚ ì§œ í¬ë§·íŒ…
        df['created_at'] = pd.to_datetime(df['created_at']).dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # CSV ì €ì¥
        filename = f"{OUTPUT_DIR}/comments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ëŒ“ê¸€ {len(df)}ê°œ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename}")
        return df
        
    except Exception as e:
        print(f"âŒ ëŒ“ê¸€ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        return None

def generate_summary_report(news_df, users_df, likes_df, views_df, comments_df):
    """ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
    print("\nğŸ“Š ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    
    report = {
        'export_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_counts': {
            'news_articles': len(news_df) if news_df is not None else 0,
            'users': len(users_df) if users_df is not None else 0,
            'likes': len(likes_df) if likes_df is not None else 0,
            'views': len(views_df) if views_df is not None else 0,
            'comments': len(comments_df) if comments_df is not None else 0
        }
    }
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜
    if news_df is not None:
        category_counts = news_df['category'].value_counts().to_dict()
        report['news_by_category'] = category_counts
    
    # í™œì„± ì‚¬ìš©ì ìˆ˜
    if users_df is not None:
        active_users = len(users_df[users_df['is_active'] == True])
        report['active_users'] = active_users
    
    # ë³´ê³ ì„œ ì €ì¥
    filename = f"{OUTPUT_DIR}/export_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {filename}")
    print("\nğŸ“ˆ ë‚´ë³´ë‚´ê¸° ìš”ì•½:")
    print(f"  â€¢ ë‰´ìŠ¤ ê¸°ì‚¬: {report['total_counts']['news_articles']}ê°œ")
    print(f"  â€¢ ì‚¬ìš©ì: {report['total_counts']['users']}ëª…")
    print(f"  â€¢ ì¢‹ì•„ìš”: {report['total_counts']['likes']}ê°œ")
    print(f"  â€¢ ì¡°íšŒ: {report['total_counts']['views']}ê°œ")
    print(f"  â€¢ ëŒ“ê¸€: {report['total_counts']['comments']}ê°œ")

def create_demo_dataset(news_df):
    """ë°ëª¨ìš© ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±"""
    if news_df is None or len(news_df) == 0:
        print("âŒ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ì–´ ë°ëª¨ ë°ì´í„°ì…‹ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("\nğŸ¯ ë°ëª¨ìš© ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  ê¸°ì‚¬ 5ê°œì”© ì„ íƒ
    demo_articles = []
    categories = news_df['category'].unique()
    
    for category in categories:
        if pd.notna(category):
            category_articles = news_df[news_df['category'] == category].head(5)
            demo_articles.append(category_articles)
    
    if demo_articles:
        demo_df = pd.concat(demo_articles, ignore_index=True)
        
        # ì¤‘ë³µ ì œê±°
        demo_df = demo_df.drop_duplicates(subset=['news_id'])
        
        # ë°ëª¨ìš© ì»¬ëŸ¼ë§Œ ì„ íƒ
        demo_columns = ['news_id', 'title', 'author', 'summary', 'updated', 'category', 'keywords']
        demo_df = demo_df[demo_columns]
        
        # CSV ì €ì¥
        filename = f"{OUTPUT_DIR}/demo_news_sample_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        demo_df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ë°ëª¨ ë°ì´í„°ì…‹ {len(demo_df)}ê°œ ìƒì„± ì™„ë£Œ: {filename}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ SSAFY News ë°ì´í„° CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘")
    print("=" * 50)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    create_output_directory()
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    conn = connect_to_db()
    if not conn:
        print("âŒ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(1)
    
    try:
        # ê° í…Œì´ë¸”ë³„ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
        news_df = export_news_articles(conn)
        users_df = export_user_data(conn)
        likes_df = export_likes_data(conn)
        views_df = export_views_data(conn)
        comments_df = export_comments_data(conn)
        
        # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        generate_summary_report(news_df, users_df, likes_df, views_df, comments_df)
        
        # ë°ëª¨ìš© ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±
        create_demo_dataset(news_df)
        
        print("\nğŸ‰ ëª¨ë“  ë°ì´í„° ë‚´ë³´ë‚´ê¸°ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ì¶œë ¥ í´ë”: {os.path.abspath(OUTPUT_DIR)}")
        
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    finally:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ
        conn.close()
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ")

if __name__ == "__main__":
    main()
